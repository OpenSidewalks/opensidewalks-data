import sys

import geopandas as gpd
import networkx as nx
import numpy as np
import pandas as pd
import rasterio as rio
import scipy
from shapely.geometry import MultiPoint, LinearRing, LineString, Point


import datahelpers as dh


WGS84 = 4326


rule all:
    input:
        "output/transportation.geojson"


# NOTE: These datasets use MultiCurve geometries to represent sidewalks (and maybe
# some crossings?), which is a real weird geometry type for that use case. I think it
# is due to their original use of street polygons. fiona will raise an
# UnsupportedGeometryTypeError (11) for geometries of this type and won't let you even
# make an attempt at parsing them yourself. So, we use ogr2ogr on these datasets.
rule extract_sidewalks:
    input:
        "data_sources/COB_Transportation.gdb"
    output:
        "interim/imported/sidewalks.geojson"
    shell:
        """
        ogr2ogr -f "GeoJSON" -nlt "MultiLineString" -t_srs "epsg:4326" {output} {input} "tran_Sidewalks"
        """


rule extract_crossings:
    input:
        "data_sources/COB_Transportation.gdb"
    output:
        "interim/imported/crossings.geojson"
    shell:
        """
        ogr2ogr -f "GeoJSON" -nlt "LineString" -t_srs "epsg:4326" {output} {input} "tran_Crosswalks"
        """


rule extract_curbramps:
    input:
        "data_sources/COB_Transportation.gdb"
    output:
        "interim/imported/curbramps.geojson"
    shell:
        """
        ogr2ogr -f "GeoJSON" -nlt "Point" -t_srs "epsg:4326" {output} {input} "tran_WheelchairRamps"
        """


rule extract_streets:
    input:
        "data_sources/COB_Transportation.gdb"
    output:
        "interim/imported/streets.geojson"
    shell:
        """
        ogr2ogr -f "GeoJSON" -nlt "LineString" -t_srs "epsg:4326" {output} {input} "tran_WhatcomRoads"
        """

rule clean_sidewalks:
    input:
        "interim/imported/sidewalks.geojson"
    output:
        "interim/clean/sidewalks.geojson"
    run:
        # The sidewalks data has all the info we need. Just need to rename columns
        # and values, filter out unused/invalid
        df = gpd.read_file(input[0])
        crs = df.crs

        column_map = {
            "STREETNAME": "street_name",
            "SURFACETYPE": "surface",
            "STREETSIDE": "side",
            "WIDTH": "width",
        }
        df = df.rename(columns=column_map)

        # Translate surface values. If no match, set to null.
        surface_map = {
            "ACP": "asphalt",
            "ACP/PCC": "asphalt",
            "PC": "concrete",
            "PCC": "concrete",
            "Wood": "wood",
        }
        df["surface"] = df["surface"].apply(lambda x: surface_map.get(x, None))

        # Translate width values from feet to meters
        df["width"] = df["width"] * 0.3048

        # Drop all unused column names
        df = df[list(column_map.values()) + ["geometry"]]

        # Attempt to explode MultiLineString geometries
        # Note: if this is bottleneck, iterrows is slow. Use apply instead or vector
        # functions.
        df = explode_multilinestrings(df)

        # Add length and default / missing values
        bounds = df.to_crs(WGS84).total_bounds
        lon = (bounds[0] + bounds[2]) / 2
        lat = (bounds[1] + bounds[3]) / 2
        utm_zone = dh.utm.lonlat_to_utm_epsg(lon, lat)
        crs = utm_zone
        df["length"] = df.to_crs(crs)["geometry"].length

        df["layer"] = 0

        df.crs = crs

        dh.io.gdf_to_geojson(df, output[0])


rule clean_streets:
    input:
        "interim/imported/streets.geojson"
    output:
        "interim/clean/streets.geojson"
    run:
        # Drop all unused columns. We don"t use any metadata now, so that"s all of
        # them except "geometry"
        df = gpd.read_file(input[0])
        crs = df.crs
        df = gpd.GeoDataFrame(df[["geometry"]])

        # Enrich with fake "layer" data
        df["layer"] = 0

        # Attempt to explode MultiLineString geometries
        # Note: if this is bottleneck, iterrows is slow. Use apply instead or vector
        # functions.
        df = explode_multilinestrings(df)
        df.crs = crs

        dh.io.gdf_to_geojson(df, output[0])


rule clean_crossings:
    input:
        "interim/imported/crossings.geojson"
    output:
        ["interim/clean/crossings.geojson",
         "interim/clean/sidewalk_links.geojson"]
    run:
        # The crossings data has info on whether the crossing is marked, but does not
        # directly have curbramp or street info. That needs to be derived later.
        df = gpd.read_file(input[0])
        crs = df.crs

        # Extract marked vs. unmarked. Ignore all other "crossings"
        type_map = {
            "M": "marked",
            "UM": "unmarked",
            "C": "link",
        }
        df["TYPE"] = df["TYPE"].apply(lambda x: type_map.get(x, None))
        df = df[~df["TYPE"].isnull()]

        # Explode MultiLinestrings
        df = explode_multilinestrings(df)

        # Restrict to useful columns
        df["marked"] = df["TYPE"].apply(lambda x: True if x == "marked" else False)

        # Add length and default / missing values
        bounds = df.to_crs(WGS84).total_bounds
        lon = (bounds[0] + bounds[2]) / 2
        lat = (bounds[1] + bounds[3]) / 2
        utm_zone = dh.utm.lonlat_to_utm_epsg(lon, lat)
        crs = utm_zone
        df["length"] = df.to_crs(crs)["geometry"].length

        # TODO: Add "street_name" property downstream?

        # Separate out "links" from the rest - these will be treated as sidewalks
        links_df = df[df["TYPE"] == "link"]

        df = df[df["TYPE"] != "link"]
        df = gpd.GeoDataFrame(df[["geometry", "marked"]])

        df.crs = crs
        links_df.crs = crs

        dh.io.gdf_to_geojson(df, output[0])
        dh.io.gdf_to_geojson(links_df, output[1])


rule clean_curbramps:
    input:
        "interim/imported/curbramps.geojson"
    output:
        "interim/clean/curbramps.geojson"
    run:
        # The curb ramp dataset has info on things like ADA compliance, etc, but we"re
        # going to ignore everything except whether it"s obstructed, and toss out
        # all metadata.
        df = gpd.read_file(input[0])
        crs = df.crs

        # "Obstruction" appears very frequently and includes non-obstructy things.
        # Need to dive into free-form text field "comments" or discard entirely.
        # df = df[df["OBSTRUCTION"] != "Yes"]

        df = gpd.GeoDataFrame(df[["geometry"]])

        df.crs = crs

        dh.io.gdf_to_geojson(df, output[0])


rule join_sidewalks:
    input:
        ["interim/clean/sidewalks.geojson",
         "interim/clean/sidewalk_links.geojson"]
    output:
        "interim/joined/sidewalks.geojson"
    run:
        sw = gpd.read_file(input[0])
        sw_links = gpd.read_file(input[1])

        crs = sw.crs

        # Throw out all info on sidewalk links - they are derived from crossings and
        # don't have anything else useful.
        sw_links = sw_links[["geometry"]]

        # Combine
        joined_df = pd.concat([sw, sw_links], sort=False)
        joined_df = gpd.GeoDataFrame(joined_df)
        joined_df.crs = sw.crs

        dh.io.gdf_to_geojson(joined_df, output[0])


rule add_curbramps_to_crossings:
    input:
        ["interim/clean/crossings.geojson",
         "interim/clean/curbramps.geojson"]
    output:
        "interim/annotated/crossings.geojson"
    run:
        df = gpd.read_file(input[0])
        cr = gpd.read_file(input[1])

        bounds = df.total_bounds
        lon = (bounds[0] + bounds[2]) / 2
        lat = (bounds[1] + bounds[3]) / 2
        utm_zone = dh.utm.lonlat_to_utm_epsg(lon, lat)

        df = df.to_crs(utm_zone)
        cr = cr.to_crs(utm_zone)

        # Count how many curb ramp points are really close to (effectively intersect)
        # a given crossing.
        DIST_THRESHOLD = 4
        for idx, row in df.iterrows():
            crossing_geom = row["geometry"]

            bounds = list(crossing_geom.bounds)
            bounds[0] -= DIST_THRESHOLD
            bounds[1] -= DIST_THRESHOLD
            bounds[2] += DIST_THRESHOLD
            bounds[3] += DIST_THRESHOLD

            hits = list(cr.sindex.intersection(bounds))
            dists = cr.iloc[hits]["geometry"].distance(crossing_geom)
            n = (dists < DIST_THRESHOLD).sum()
            df.loc[idx, "n_curbramps"] = n

        # If curb ramp count > 1, set curb ramp flag to yes.
        df["curbramps"] = df["n_curbramps"] > 1
        df["curbramps"] = df["curbramps"].astype(int)
        df["length"] = df["geometry"].length

        df = df.to_crs(WGS84)

        dh.io.gdf_to_geojson(df, output[0])


rule intersect_sidewalks:
    input:
        "interim/joined/sidewalks.geojson"
    output:
        "interim/intersected/sidewalks_self.geojson"
    run:
        df = gpd.read_file(input[0])

        # Convert to UTM so everything can be calculated in meters
        df = dh.utm.gdf_to_utm(df)
        crs = df.crs

        # 3-step plan:
        # 1) Find all endpoint-endpoint relationships to snap, store in dictionary(s)
        #    for easy lookup later on. These endpoints will all be readjusted to an
        #    appropriate shared value.
        # 2) Find all endpoint-line relationships to snap. That is, one endpoint should
        #    snap to part of another line, but not the exact endpoint of that other
        #    line. This implies (1) moving one end of one line and (2) splitting
        #    another line.
        # 3) Find all "true" intersections between lines. Endpoints will be ruled out
        #    using steps 1 and 2. Intersections imply a new shared point and splitting
        #    2 lines.

        # TODO: optimize. These are candidates for optimization:
        # 1) Replace .iterrows() with .apply functions

        DIST_THRESHOLD = 0.1
        PRECISION = 1

        df["split"] = df["geometry"].apply(lambda x: [])

        # Step 1: endpoint-endpoint snapping. Use simple strategy of rounding to a
        # certain precision (in this case, 1 = 1 decimal place, or 0.1 meters)
        starts = pd.DataFrame({
            "coordinates": df["geometry"].apply(lambda x: tuple(np.round(x.coords[0], PRECISION))),
            "idx": df.index,
            "type": "start",
        })
        lasts = pd.DataFrame({
            "coordinates": df["geometry"].apply(lambda x: tuple(np.round(x.coords[-1], PRECISION))),
            "idx": df.index,
            "type": "last",
        })
        ends = pd.concat([starts, lasts], sort=False)
        for coord, grp in ends.groupby("coordinates").groups:
            if grp.shape:
                # We"ve got grouped-together endpoints! These should be merged to a new
                # value. Save this new value back to a new column in the original df.
                new_x = grp.apply(lambda x: x[0]).mean()
                new_y = grp.apply(lambda x: x[1]).mean()
                starts.loc[grp[grp["type"] == "start"]["idx"], "coordinates"] = (new_x, new_y)
                lasts.loc[grp[grp["type"] == "last"]["idx"], "coordinates"] = (new_x, new_y)
        ends = pd.concat([starts, lasts], sort=False)
        ends["geometry"] = ends["coordinates"].apply(lambda x: Point(x))
        ends = gpd.GeoDataFrame(ends)

        # "ends" is now a GeoDataFrame of potentially-adjusted end coordinates. We will
        # use this data structure to query nearby lines - but throw out any that are
        # just duplicate "hits" for endpoints. The remaining lines need to be split.

        # Step 2: Update the lines with new ends. When it comes time to split any
        # sidewalk lines, it will be advantageous to use the "updated" data, avoiding
        # the need to track which pieces came from which sidewalks.
        for sw_idx, grp in ends.groupby("idx"):
            coords = list(df.loc[sw_idx, "geometry"].coords)
            for idx, row in grp.iterrows():
                if row["type"] == "start":
                    coords[0] = row["coordinates"]
                else:
                    coords[-1] = row["coordinates"]
            df.loc[sw_idx, "geometry"] = LineString(coords)

        # Step 3: query for places where endpoints meet lines, implying that the line
        # needs to be split.
        for idx, row in ends.iterrows():
            point = row["geometry"]

            bounds = []
            bounds.append(point.x - DIST_THRESHOLD)
            bounds.append(point.y - DIST_THRESHOLD)
            bounds.append(point.x + DIST_THRESHOLD)
            bounds.append(point.y + DIST_THRESHOLD)

            sw_rows = df.iloc[list(df.sindex.intersection(bounds))]
            sw_rows = sw_rows[sw_rows.distance(point) < DIST_THRESHOLD]

            lengths = sw_rows["geometry"].length
            distances_along = sw_rows.project(point)
            not_starts = distances_along > (DIST_THRESHOLD * 2)
            not_lasts = distances_along < (lengths - DIST_THRESHOLD * 2)
            not_ends = not_starts & not_lasts

            if not_ends.any():
                # Grab the closest one and split it
                sorted_distances = distances_along[not_ends].sort_values()
                idx2 = sorted_distances.index[0]
                distance = sorted_distances.iloc[0]

                # There are non-endpoint hits - these geometries should be split at
                # the projected point(s).
                df.loc[idx2, "split"].append(distance)

                # The original point should also be set to the projected point
                new_point = df.loc[idx2, "geometry"].interpolate(distance)
                ends.loc[idx, "geometry"] = new_point

        # Step 4: Find line-line intersections
        ixns = []
        for idx, row in df.iterrows():
            df_matches = df.iloc[list(df.sindex.intersection(row.geometry.bounds))]
            df_sindex_matches = df_matches[df_matches.index != idx]
            intersects = df_sindex_matches.intersects(row.geometry)
            if intersects.any():
                df_matches = df_sindex_matches[intersects]

                # Now we have a small data frame of truly (non-self) intersecting sidewalks
                # Take the point at which to split and toss it on the pile

                # Points of intersection
                points = list(df_matches["geometry"].apply(lambda x: x.intersection(row.geometry)))

                # Distance along
                for point in points:
                    if point.type == "Point":
                        ixns.append(point)
                        dist_along = row["geometry"].project(point)
                        df.loc[idx, "split"].append(dist_along)

        test = gpd.GeoDataFrame(geometry=ixns)
        test.crs = crs
        dh.io.gdf_to_geojson(test, "test.geojson")

        # Step 5: split the lines
        new_rows = []
        need_splitting = df[df["split"].apply(len) > 0]
        for idx, row in need_splitting.iterrows():
            geometry = row["geometry"]
            for distance in sorted(set(row["split"]), reverse=True):
                x = dh.geometry.cut(geometry, distance)
                geoms = dh.geometry.cut(geometry, distance)
                if len(geoms) < 2:
                    # There has been an error, probably duplicate points - skip cutting
                    continue
                geometry, tail = geoms
                new_row = df.loc[idx].copy(deep=True)
                new_row["geometry"] = tail
                new_rows.append(new_row)
            df.loc[idx, "geometry"] = geometry

            # Drop the pre-split
            df.drop(idx)

        new_df = gpd.GeoDataFrame(new_rows)

        combined_df = gpd.GeoDataFrame(pd.concat([df, new_df], sort=False))
        combined_df.crs = crs
        df_wgs84 = combined_df.to_crs(WGS84)

        # FIXME: Need to snap endpoints again - they are not properly-organized/shared
        # and, in particular, crossings + sidewalks end up just far enough away from
        # one another to break the graph

        # FIXME: Short paths connecting to crossings should be set to zero incline

        dh.io.gdf_to_geojson(df_wgs84, output[0])


rule intersect_sidewalks_crossings:
    input:
        ["interim/intersected/sidewalks_self.geojson",
         "interim/annotated/crossings.geojson"]
    output:
        "interim/intersected/sidewalks.geojson"
    run:
        sw = gpd.read_file(input[0])
        cr = gpd.read_file(input[1])

        # Convert to UTM so everything can be calculated in meters
        sw = dh.utm.gdf_to_utm(sw)
        cr = dh.utm.gdf_to_utm(cr)

        crs = sw.crs

        # Plan:
        # 1) Find crossing endpoint-sidewalk lines to snap. Record distance along the
        #    sidewalk to "snap"
        # 2) Split the sidewalks at those points

        DIST_THRESHOLD = 0.1
        PRECISION = 1

        sw["split"] = sw["geometry"].apply(lambda x: [])

        # Step 1: crossing endpoint - sidewalk line snapping.
        starts = pd.DataFrame({
            "coordinates": cr["geometry"].apply(lambda x: tuple(np.round(x.coords[0], PRECISION))),
        })
        lasts = pd.DataFrame({
            "coordinates": cr["geometry"].apply(lambda x: tuple(np.round(x.coords[-1], PRECISION))),
        })
        ends = pd.concat([starts, lasts], sort=False)
        ends["geometry"] = ends["coordinates"].apply(lambda x: Point(x))
        ends = gpd.GeoDataFrame(ends)

        # FIXME: we should probably edit the crossings as well, to ensure they meet
        # end-to-end within a very small delta.

        # Query for places where endpoints meet lines, implying that the line needs to
        # be split.
        for idx, row in ends.iterrows():
            point = row["geometry"]

            bounds = []
            bounds.append(point.x - DIST_THRESHOLD)
            bounds.append(point.y - DIST_THRESHOLD)
            bounds.append(point.x + DIST_THRESHOLD)
            bounds.append(point.y + DIST_THRESHOLD)

            sw_rows = sw.iloc[list(sw.sindex.intersection(bounds))]
            sw_rows = sw_rows[sw_rows.distance(point) < DIST_THRESHOLD]

            lengths = sw_rows["geometry"].length
            distances_along = sw_rows.project(point)
            not_starts = distances_along > (DIST_THRESHOLD * 2)
            not_lasts = distances_along < (lengths - DIST_THRESHOLD * 2)
            not_ends = not_starts & not_lasts

            if not_ends.any():
                # Grab the closest one and split it
                sorted_distances = distances_along[not_ends].sort_values()
                idx2 = sorted_distances.index[0]
                distance = sorted_distances.iloc[0]

                # There are non-endpoint hits - these geometries should be split at
                # the projected point(s).
                sw.loc[idx2, "split"].append(distance)

        # Step 3: split the lines
        new_rows = []
        need_splitting = sw[sw["split"].apply(len) > 0]
        for idx, row in need_splitting.iterrows():
            geometry = row["geometry"]
            for distance in sorted(set(row["split"]), reverse=True):
                # print("--------------")
                # print(geometry.wkt)
                # print(list(geometry.coords))
                # print(geometry.length, distance)
                x = dh.geometry.cut(geometry, distance)
                geometry, tail = dh.geometry.cut(geometry, distance)
                new_row = sw.loc[idx].copy(deep=True)
                new_row["geometry"] = tail
                new_rows.append(new_row)
            sw.loc[idx, "geometry"] = geometry

        new_sw = gpd.GeoDataFrame(new_rows)

        # FIXME: do we need to drop the "old" rows?
        combined_df = gpd.GeoDataFrame(pd.concat([sw, new_sw], sort=False))
        combined_df.crs = crs
        sw_wgs84 = combined_df.to_crs(WGS84)

        # FIXME: there are precision issues with these snapped coordinates: they are up
        # to ~7 mm away from one another. This isn"t an issue in visualization, but our
        # routing graph requires virtually perfect precision: the 7th lon-lat decimal
        # place, which counts for ~11 mm depending on the lat.

        # The routing server has been rounded down to a precision of 6 in the meantime

        dh.io.gdf_to_geojson(sw_wgs84, output[0])


rule intersection_elevations:
    input:
        ["data_sources/dem.tif",
         "interim/clean/streets.geojson",
         "interim/intersected/sidewalks.geojson"]
    output:
        "interim/dem/intersection_elevations.geojson"
    run:
        dem = rio.open(input[0])
        st = gpd.read_file(input[1])
        sw = gpd.read_file(input[2])

        # Use sidewalks extent to limit streets.
        # TODO: consider having a hard-coded extent polygon / bounding box per-city
        bbox = sw.total_bounds
        st = st.iloc[list(st.sindex.intersection(bbox))]

        st_dem = st.to_crs(dem.crs.to_epsg())

        # Create a graph from the streets
        G = nx.Graph()
        for idx, row in st.iterrows():
            coords = row.geometry.coords
            start = np.round(coords[0], 6)
            end = np.round(coords[-1], 6)

            node_start = str(start)
            node_end = str(end)

            G.add_node(node_start, x=start[0], y=start[1])
            G.add_node(node_end, x=end[0], y=end[1])
            # Retain orientation information
            G.add_edge(node_start, node_end, start=node_start,
                       geometry=row.geometry,
                       geometry_dem=st_dem.loc[idx, "geometry"])

        # Create the geometries for the mask - intersections extended a small
        # distance
        rows = []
        n = 0
        for node, degree in G.degree:
            if (degree == 1) or (degree > 2):
                n += 1
                # It"s an intersection or a dead end
                for u, v, d in G.edges(node, data=True):
                    geom = d["geometry"]
                    geom_dem = d["geometry_dem"]
                    if u == d["start"]:
                        x, y = geom.coords[0]
                        x_dem, y_dem = geom_dem.coords[0]
                    else:
                        x, y = geom.coords[-1]
                        x_dem, y_dem = geom_dem.coords[-1]
                    try:
                        elevation = dh.raster_interp.interpolated_value(x_dem, y_dem, dem)
                    except Exception as e:
                        print(x_dem)
                        print(y_dem)
                        raise e

                    rows.append({
                        "geometry": Point(x, y),
                        "elevation": elevation
                    })

        gdf = gpd.GeoDataFrame(rows)
        dh.io.gdf_to_geojson(gdf, output[0])


rule add_inclines:
    input:
        ["interim/intersected/sidewalks.geojson",
         "interim/dem/intersection_elevations.geojson",
         "interim/joined/sidewalks.geojson"]
    output:
        "interim/inclined/sidewalks.geojson"
    run:
        sw = gpd.read_file(input[0])
        el = gpd.read_file(input[1])

        el["x"] = el.geometry.apply(lambda p: p.x)
        el["y"] = el.geometry.apply(lambda p: p.y)

        convex_hull = LinearRing(MultiPoint(el.geometry).convex_hull.exterior.coords)

        interpolate = scipy.interpolate.LinearNDInterpolator(el[["x", "y"]],
                                                             el["elevation"],
                                                             fill_value=-1000)

        sw["ele_start"] = sw.geometry.apply(lambda l: interpolate(*l.coords[0]))
        sw["ele_end"] = sw.geometry.apply(lambda l: interpolate(*l.coords[-1]))

        bounds = sw.total_bounds
        lon = (bounds[0] + bounds[2]) / 2
        lat = (bounds[1] + bounds[3]) / 2
        utm_zone = dh.utm.lonlat_to_utm_epsg(lon, lat)
        sw["len"] = sw.to_crs(utm_zone).geometry.length

        # Remove zero-length geometries. These are cases where the start and end
        # points are the same. For Bellingham, these apply to very short and
        # disconnected lines that might just be artifacts of some other process.
        sw = sw[sw["len"] != 0]

        # If interpolated elevation is -1000, that means we just failed to
        # interpolate at all. We should "snap" that point to the nearest valid
        # section of the interpolator, which is a convex hull of the
        # intersections.
        missed = sw.loc[(sw.ele_start == -1000) | (sw.ele_end == -1000)]
        for idx, row in missed.iterrows():
            factor = 1
            if row.ele_start == -1000:
                start = Point(row.geometry.coords[0])
                proj_start = convex_hull.interpolate(convex_hull.project(start))

                dx = (proj_start.x - start.x)
                dy = (proj_start.y - start.y)
                len = dx**2 + dy**2
                dx = factor * dx / len
                dy = factor * dy / len
                x = proj_start.x + dx
                y = proj_start.y + dy
                point_start = Point(x, y)
                sw.loc[idx, "ele_start"] = interpolate(*point_start.coords)

            if row.ele_end == -1000:
                end = Point(row.geometry.coords[-1])
                proj_end = convex_hull.interpolate(convex_hull.project(end))
                dx = (proj_end.x - end.x)
                dy = (proj_end.y - end.y)
                len = dx**2 + dy**2
                dx = factor * dx / len
                dy = factor * dy / len
                x = proj_end.x + dx
                y = proj_end.y + dy
                point_end = Point(x, y)
                sw.loc[idx, "ele_end"] = interpolate(*point_end.coords)

        # If there"s still some missing, just snap to the closest
        missed = sw.loc[(sw.ele_start == -1000) | (sw.ele_end == -1000)]
        for idx, row in missed.iterrows():
            if row.ele_start == -1000:
                start = Point(row.geometry.coords[0])
                idx2 = el.distance(start).sort_values().index[0]
                sw.loc[idx, "ele_start"] = el.loc[idx2, "elevation"]

            if row.ele_end == -1000:
                end = Point(row.geometry.coords[-1])
                idx2 = el.distance(end).sort_values().index[0]
                sw.loc[idx, "ele_end"] = el.loc[idx2, "elevation"]

        sw["incline"] = (sw.ele_end - sw.ele_start) / sw.len
        # sw = sw.drop(columns=["ele_start", "ele_end", "len"])

        # Convert to integer, keep in range [-9999, 9999]

        sw.incline = round(sw.incline, 3)
        sw.incline = sw.incline.apply(lambda x: min(max(x, -.99), .99))

        sw = sw.to_crs(WGS84)

        dh.io.gdf_to_geojson(sw, output[0])


rule flatten_intersections:
    input:
        ["interim/inclined/sidewalks.geojson",
         "interim/annotated/crossings.geojson"]
    output:
        "interim/flattened/sidewalks.geojson"
    run:
        MIN_LENGTH = 1
        MAX_LENGTH = 5
        THRESHOLD = 0.1

        sw = gpd.read_file(input[0])
        cr = gpd.read_file(input[1])

        sw = dh.utm.gdf_to_utm(sw)
        cr = dh.utm.gdf_to_utm(cr)

        # Strategy:
        # (1) Figure out which sidewalk lines are touching crossings (by distance)
        # (2) Sidewalks that touch crossings and fall below minimum length (e.g. 3
        #     meters) will have their incline set to zero.

        for idx, row in sw.iterrows():
            if row["geometry"].length > MAX_LENGTH:
                # This is a long geom - use the estimated incline
                continue
            if row["geometry"].length < MIN_LENGTH:
                # This is a very short geom - the estimated incline can"t be trusted
                # FIXME: this is a workaround for short segments near intersections,
                # in addition to the general concern noted above. This might be better
                # addressed by a network-based approach or improvements to the incline
                # estimator
                sw.loc[idx, "incline"] = 0
                continue
            cr_bbox = cr.iloc[list(cr.sindex.intersection(row["geometry"].bounds))]
            if (cr_bbox.distance(row["geometry"]) < THRESHOLD).any():
                sw.loc[idx, "incline"] = 0

        sw = sw.to_crs(WGS84)

        dh.io.gdf_to_geojson(sw, output[0])


rule snap:
    input:
        ["interim/flattened/sidewalks.geojson",
         "interim/annotated/crossings.geojson"]
    output:
        ["interim/snapped/sidewalks.geojson",
         "interim/snapped/crossings.geojson"]
    run:
        sw = gpd.read_file(input[0])
        cr = gpd.read_file(input[1])

        THRESHOLD1 = 0.1

        #
        # Snap the endpoints of the dataset together within a threshold.
        #
        # TODO: work into a helper function, this is reusable

        # Combine into single GeoDataFrame for clustering of endpoints
        datasets = [sw, cr]
        combined = []
        for i, dataset in enumerate(datasets):
            copy = dataset[["geometry"]].copy(deep=True)
            copy["_dataset_i"] = i
            combined.append(copy)

        df = pd.concat(combined, sort=False)

        # Reset the index so every row has a unique entry
        df = df.reset_index(drop=True)

        # Convert to UTM so everything is in meters
        df = dh.utm.gdf_to_utm(df)

        # Clustering strategy:
        # 1) Clusters start as every single endpoint, i.e. there are 2 * rows of them
        # 2) Use spatial index with a small box to start grouping clusters together,
        #    i.e. making new ones. Will "touch" all points until they"ve all been
        #    regrouped. Grouping will be, essentially, recursive.
        # 3) TODO: As a final clean-up, there *might* be a final clustering attempt
        #    with a slightly higher distance threshold.
        # TODO: this strategy is very slow. Haven"t profiled it, but something about it
        # is much slower than expected.
        # IDEA: Create "buffered" bounding boxes of endpoints and create a graph based
        # on spatial index matches: i <-> j edges. Connected subgraphs are clusters.
        starts = df.geometry.apply(lambda x: Point(x.coords[0]))
        lasts = df.geometry.apply(lambda x: Point(x.coords[-1]))
        ends = gpd.GeoDataFrame(geometry=pd.concat([starts, lasts], sort=False))
        ends["cluster"] = None
        start_labels = [1 for i in range(df.shape[0])]
        end_labels = [0 for i in range(df.shape[0])]
        ends["start"] = start_labels + end_labels
        ends["df_idx"] = 2 * list(df.index)
        # Reindex: the starts/lasts duplicate the main table"s index
        ends = ends.reset_index(drop=True)

        #
        # Cluster!
        #
        cluster_n = 0
        # Note: there are a lot of redundant calculations done here, but the spatial
        # index may make it plenty fast
        for idx, end in ends.iterrows():
            bounds = list(end["geometry"].bounds)
            bounds[0] -= THRESHOLD1
            bounds[1] -= THRESHOLD1
            bounds[2] += THRESHOLD1
            bounds[3] += THRESHOLD1

            candidate_idxs = list(ends.sindex.intersection(bounds))
            candidate_locs = ends.index[candidate_idxs]
            candidates = ends.iloc[candidate_idxs]

            # If the rows are already in cluster(s), merge the clusters and assign all to
            # the lowest-number cluster
            has_cluster = candidates[~candidates["cluster"].isnull()]
            if has_cluster.shape[0]:
                # There are already clusters!
                cluster_id = has_cluster["cluster"].min()
            else:
                cluster_id = cluster_n
                cluster_n += 1
            ends.loc[candidate_locs, "cluster"] = cluster_id

        for cluster_id, grp in ends.groupby("cluster"):
            if grp.shape[0] < 2:
                # There"s only one point in this group - no need to change.
                continue
            # Average the point coordinates
            xs = grp["geometry"].apply(lambda p: p.x)
            ys = grp["geometry"].apply(lambda p: p.y)
            x = xs.mean()
            y = ys.mean()
            geom = Point(x, y)

            for idx, row in grp.iterrows():
                coords = list(df.loc[row["df_idx"]]["geometry"].coords)
                if row["start"]:
                    coords[0] = geom
                else:
                    coords[-1] = geom

                df.loc[row["df_idx"], "geometry"] = LineString(coords)


        df = df.to_crs(WGS84)
        for i, dataset in enumerate(datasets):
            dataset["geometry"] = list(df[df["_dataset_i"] == i]["geometry"])
            # Clean up - some geometries may have been invalidated by moving coordinates
            dataset = dataset.loc[dataset.geometry.length != 0]
            # Just in case
            datasets[i] = dataset

        dh.io.gdf_to_geojson(datasets[0], output[0])
        dh.io.gdf_to_geojson(datasets[1], output[1])


rule finalize:
    input:
        ["interim/snapped/sidewalks.geojson",
         "interim/snapped/crossings.geojson"]
    output:
        expand("interim/finalized/{layer}.geojson", layer=["sidewalks", "crossings"])
    run:
        # Convert to lon-lat and put in output directory
        for (in_path, out_path) in zip(input, output):
            df = gpd.read_file(in_path)
            df = df.to_crs(WGS84)
            dh.io.gdf_to_geojson(df, out_path)


rule standardize:
    input:
        ["interim/finalized/sidewalks.geojson",
         "interim/finalized/crossings.geojson"]
    output:
        "output/transportation.geojson"
    run:
        transportation = {
            "type": "FeatureCollection",
            "features": []
        }

        # Standardize sidewalks
        with open(input[0]) as f:
            sw = json.load(f)
        for feature in sw["features"]:
            props = feature["properties"]
            new_props = {}
            new_props["subclass"] = "footway"
            new_props["footway"] = "sidewalk"
            # Calculate new lengths using Great-Circle Distance formula
            length = dh.haversine(feature["geometry"]["coordinates"])
            new_props["length"] = round(length, 1)
            if "incline" in props:
                new_props["incline"] = props["incline"]
            if "surface" in props:
                new_props["surface"] = props["surface"]
            if "width" in props:
                new_props["width"] = props["width"]
            if "layer" in props:
                new_props["layer"] = props["layer"]
            if "side" in props and "street_name" in props:
                new_props["description"] = "Sidewalk {} of {}".format(props["side"], props["street_name"])

            transportation["features"].append({
                "type": "Feature",
                "geometry": feature["geometry"],
                "properties": new_props
            })

        # Standardize crossings
        with open(input[1]) as f:
            cr = json.load(f)
        for feature in cr["features"]:
            props = feature["properties"]
            new_props = {}
            new_props["subclass"] = "footway"
            new_props["footway"] = "crossing"
            if "marked" in props:
                if props["marked"]:
                    new_props["crossing"] = "marked"
                else:
                    new_props["crossing"] = "unmarked"
            if "curbramps" in props:
                new_props["curbramps"] = props["curbramps"]
            if "length" in props:
                new_props["length"] = props["length"]
            if "street_name" in props:
                new_props["description"] = "Crossing at {}".format(props["street_name"])

            transportation["features"].append({
                "type": "Feature",
                "geometry": feature["geometry"],
                "properties": new_props
            })

        with open(output[0], "w") as g:
            json.dump(transportation, g)


def explode_multilinestrings(df):
    # Attempt to explode MultiLineString geometries
    # Note: if this is bottleneck, iterrows is slow. Use apply instead or vector
    # functions.
    new_rows = []
    for idx, row in df.iterrows():
        if row["geometry"].type == "MultiLineString":
            for geom in row["geometry"].geoms:
                new_row = dict(row)
                new_row["geometry"] = geom
                new_rows.append(new_row)
        else:
            new_rows.append(dict(row))

    new = gpd.GeoDataFrame(new_rows)
    new.crs = df.crs
    return new
